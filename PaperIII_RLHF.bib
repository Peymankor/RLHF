@online{25Aug2023,
  title = {[25 {{Aug}} 2023 {{DLCT}} 50min] {{Objective Mismatch}} in {{RLHF}}.Pdf},
  url = {https://drive.google.com/file/d/1F1lIi48PrWl5_80wQF40Y5dQmCBiZUUh/view?usp=embed_facebook},
  urldate = {2023-08-26},
  organization = {Google Docs},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/783QE23Z/view.html}
}

@online{39004369ProxyObjectives,
  title = {39004369 · {{Proxy}} Objectives in Reinforcement Learning from Human Feedback},
  url = {https://slideslive.com/embed/presentation/39004369?js_embed_version=3&embed_init_token=eyJhbGciOiJIUzI1NiJ9.eyJpYXQiOjE3MzE1MTkzODQsImV4cCI6MTczMTY0ODk4NCwidSI6eyJ1dWlkIjoiNTU2MjliMGMtZTM2ZS00NmQ3LWE5ZjItYTEwZjA0Y2M1Mjg3IiwiaSI6bnVsbCwiZSI6bnVsbCwibSI6ZmFsc2V9LCJkIjoiaWNtbC5jYyJ9.Hq7rF4YdO2l_PRFKlSqWSeEzigaa-rWC69X4Hoi0QyQ&embed_parent_url=https%3A%2F%2Ficml.cc%2Fvirtual%2F2023%2Finvited-talk%2F21549&embed_origin=https%3A%2F%2Ficml.cc&embed_container_id=presentation-embed-39004369&auto_load=true&auto_play=false&zoom_ratio=&disable_fullscreen=false&locale=en&vertical_enabled=true&vertical_enabled_on_mobile=false&allow_hidden_controls_when_paused=true&fit_to_viewport=true&custom_user_id=&user_uuid=55629b0c-e36e-46d7-a9f2-a10f04cc5287},
  urldate = {2024-11-13},
  abstract = {Professional Conference Recording},
  langid = {american},
  organization = {SlidesLive},
  keywords = {To Read}
}

@online{aerinRLHFReinforcementLearning2023,
  title = {{{RLHF}}: {{Reinforcement Learning}} from {{Human Feedback}}},
  shorttitle = {{{RLHF}}},
  author = {Aerin, Ms},
  date = {2023-10-20T14:16:37},
  url = {https://towardsdatascience.com/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1},
  urldate = {2023-10-23},
  abstract = {ChatGPT’s success ingredient: The Instruction Data.},
  langid = {english},
  organization = {Medium},
  keywords = {To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/CUBLEE9G/rlhf-reinforcement-learning-from-human-feedback-faa5ff4761d1.html}
}

@article{anandReinforcementLearningFinetuning2022,
  title = {Reinforcement {{Learning}} as a Fine-Tuning Paradigm},
  author = {Anand, Ankesh},
  date = {2022-01-08},
  journaltitle = {ankeshanand.com},
  url = {http://ankeshanand.com/blog/2022/01/08/rl-fine-tuning.html},
  urldate = {2023-08-28},
  abstract = {Reinforcement Learning should be better seen as a “fine-tuning” paradigm that can add capabilities to general-purpose foundation models, rather than a paradigm that can bootstrap intelligence from scratch.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/GJBHGB8F/rl-fine-tuning.html}
}

@online{andrejkarpathy[karpathy]RLHFJustBarely2024,
  type = {Tweet},
  title = {{{RLHF}} Is Just Barely {{RL Reinforcement Learning}} from {{Human Feedback}} ({{RLHF}}) Is the Third (and Last) Major Stage of Training an {{LLM}}, after Pretraining and Supervised Finetuning ({{SFT}}). {{My}} Rant on {{RLHF}} Is That It Is Just Barely {{RL}}, in a Way That {{I}} Think Is Not Too Widely {{https://t.co/sjRZvqc5KC}}},
  author = {{Andrej Karpathy [karpathy]}},
  date = {2024-08-07T20:08Z},
  url = {https://twitter.com/karpathy/status/1821277264996352246},
  urldate = {2024-08-08},
  langid = {english},
  organization = {Twitter},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/MBLIT7ZJ/1821277264996352246.html}
}

@online{askellGeneralLanguageAssistant2021,
  title = {A {{General Language Assistant}} as a {{Laboratory}} for {{Alignment}}},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  date = {2021-12-09},
  eprint = {2112.00861},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.00861},
  url = {http://arxiv.org/abs/2112.00861},
  urldate = {2024-09-24},
  abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/532LAFBC/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/LWZA2FZS/2112.html}
}

@online{atariWhichHumans2023,
  title = {Which {{Humans}}?},
  author = {Atari, Mohammad and Xue, Mona J. and Park, Peter S. and Blasi, Damián and Henrich, Joseph},
  date = {2023-09-22},
  eprinttype = {OSF},
  doi = {10.31234/osf.io/5b26t},
  url = {https://osf.io/5b26t},
  urldate = {2024-10-25},
  abstract = {Large language models (LLMs) have recently made vast advances in both generating and analyzing textual data. Technical reports often compare LLMs’ outputs with “human” performance on various tests. Here, we ask, “Which humans?” Much of the existing literature largely ignores the fact that humans are a cultural species with substantial psychological diversity around the globe that is not fully captured by the textual data on which current LLMs have been trained. We show that LLMs’ responses to psychological measures are an outlier compared with large-scale cross-cultural data, and that their performance on cognitive psychological tasks most resembles that of people from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies but declines rapidly as we move away from these populations (r = -.70). Ignoring cross-cultural diversity in both human and machine psychology raises numerous scientific and ethical issues. We close by discussing ways to mitigate the WEIRD bias in future generations of generative language models.},
  langid = {american},
  pubstate = {prepublished},
  keywords = {Artificial Intelligence,Culture,Human Psychology,Large Language Models,Machine Psychology,Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/XV3F8D4H/which_humans_09222023.pdf}
}

@online{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and Telleen-Lawton, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  date = {2022-12-15},
  eprint = {2212.08073},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2023-07-19},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/Q8Y2PBSX/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/G85FPWKI/2212.html}
}

@online{baiTrainingHelpfulHarmless2022a,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  date = {2022-04-12},
  eprint = {2204.05862},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2204.05862},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2024-11-17},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/JRQAID3C/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/MR95SYYJ/2204.html}
}

@inproceedings{benderDangersStochasticParrots2021,
  title = {On the {{Dangers}} of {{Stochastic Parrots}}: {{Can Language Models Be Too Big}}? 🦜},
  shorttitle = {On the {{Dangers}} of {{Stochastic Parrots}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  date = {2021-03-01},
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  urldate = {2024-09-18},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  keywords = {Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/XUG7JLJN/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf}
}

@online{BuildlanguagemodelsawsAlignmodelsamazonsagemakerRlhfmultiadapterppo,
  title = {Build-Language-Models-on-Aws/Align-Models-with-Amazon-Sagemaker/Rlhf-with-Multi-Adapter-Ppo at Main · Aws-Samples/Build-Language-Models-on-Aws},
  url = {https://github.com/aws-samples/build-language-models-on-aws/tree/main/align-models-with-amazon-sagemaker/rlhf-with-multi-adapter-ppo},
  urldate = {2024-11-08},
  abstract = {Contribute to aws-samples/build-language-models-on-aws development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/4YPIA457/rlhf-with-multi-adapter-ppo.html}
}

@online{casperOpenProblemsFundamental2023,
  title = {Open {{Problems}} and {{Fundamental Limitations}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Raphaël and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Bıyık, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
  date = {2023-07-27},
  eprint = {2307.15217},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.15217},
  url = {http://arxiv.org/abs/2307.15217},
  urldate = {2023-07-31},
  abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/8PV72IQF/Casper et al. - 2023 - Open Problems and Fundamental Limitations of Reinf.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/N3R2N52Y/2307.html}
}

@inproceedings{christianoDeepReinforcementLearning2017,
  title = {Deep {{Reinforcement Learning}} from {{Human Preferences}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  date = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html},
  urldate = {2024-09-18},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. Our approach separates learning the goal from learning the behavior to achieve it. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on about 0.1\% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.},
  keywords = {Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/CA4E44V2/Christiano et al. - 2017 - Deep Reinforcement Learning from Human Preferences.pdf}
}

@software{ContextualAIHALOs2024,
  title = {{{ContextualAI}}/{{HALOs}}},
  date = {2024-10-11T11:22:37Z},
  origdate = {2023-12-03T07:53:36Z},
  url = {https://github.com/ContextualAI/HALOs},
  urldate = {2024-10-11},
  abstract = {A library with extensible implementations of DPO, KTO, PPO, ORPO, and other human-aware loss functions (HALOs).},
  organization = {ContextualAI},
  keywords = {alignment,dpo,halos,kto,ppo,rlhf}
}

@online{DecodingStrategiesLarge,
  title = {Decoding {{Strategies}} in {{Large Language Models}}},
  url = {https://huggingface.co/blog/mlabonne/decoding-strategies},
  urldate = {2024-10-30},
  abstract = {A Blog post by Maxime Labonne on Hugging Face},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/Y7Y92QBS/decoding-strategies.html}
}

@online{DetoxifyingLanguageModel,
  title = {Detoxifying a {{Language Model}} Using {{PPO}}},
  url = {https://huggingface.co/docs/trl/detoxifying_a_lm},
  urldate = {2024-11-03},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/ZFELMHEC/detoxifying_a_lm.html}
}

@online{ethayarajhKTOModelAlignment2024,
  title = {{{KTO}}: {{Model Alignment}} as {{Prospect Theoretic Optimization}}},
  shorttitle = {{{KTO}}},
  author = {Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  date = {2024-09-03},
  eprint = {2402.01306},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01306},
  url = {http://arxiv.org/abs/2402.01306},
  urldate = {2024-09-15},
  abstract = {Kahneman \& Tversky's \$\textbackslash textit\{prospect theory\}\$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call \$\textbackslash textit\{human-aware losses\}\$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/R859C4BT/Ethayarajh et al. - 2024 - KTO Model Alignment as Prospect Theoretic Optimiz.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/994XME86/2402.html}
}

@article{fernandesBridgingGapSurvey2023,
  title = {Bridging the {{Gap}}: {{A Survey}} on {{Integrating}} ({{Human}}) {{Feedback}} for {{Natural Language Generation}}},
  shorttitle = {Bridging the {{Gap}}},
  author = {Fernandes, Patrick and Madaan, Aman and Liu, Emmy and Farinhas, António and Martins, Pedro Henrique and Bertsch, Amanda and family=Souza, given=José G. C., prefix=de, useprefix=true and Zhou, Shuyan and Wu, Tongshuang and Neubig, Graham and Martins, André F. T.},
  date = {2023-12-19},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  shortjournal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1643--1668},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00626},
  url = {https://doi.org/10.1162/tacl_a_00626},
  urldate = {2024-11-21},
  abstract = {Natural language generation has witnessed significant advancements due to the training of large language models on vast internet-scale datasets. Despite these advancements, there exists a critical challenge: These models can inadvertently generate content that is toxic, inaccurate, and unhelpful, and existing automatic evaluation metrics often fall short of identifying these shortcomings. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of recent research that has leveraged human feedback to improve natural language generation. First, we introduce a taxonomy distilled from existing research to categorize and organize the varied forms of feedback. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which uses large language models to make judgments based on a set of principles and minimize the need for human intervention. We also release a website of this survey at feedback-gap-survey.info.},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/NZMSAHZ9/Fernandes et al. - 2023 - Bridging the Gap A Survey on Integrating (Human) Feedback for Natural Language Generation.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/3L84V8H3/118795.html}
}

@article{gabrielArtificialIntelligenceValues2020,
  title = {Artificial {{Intelligence}}, {{Values}}, and {{Alignment}}},
  author = {Gabriel, Iason},
  date = {2020-09-01},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  volume = {30},
  number = {3},
  pages = {411--437},
  issn = {1572-8641},
  doi = {10.1007/s11023-020-09539-2},
  url = {https://doi.org/10.1007/s11023-020-09539-2},
  urldate = {2024-09-24},
  abstract = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify ‘true’ moral principles for AI; rather, it is to identify fair principles for alignment that receive reflective endorsement despite widespread variation in people’s moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
  langid = {english},
  keywords = {Artificial intelligence,Artificial Intelligence,Machine learning,Medical Ethics,Moral philosophy,Political theory,Read,Value alignment},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/HF7PTL5E/Gabriel - 2020 - Artificial Intelligence, Values, and Alignment.pdf}
}

@online{gehmanRealToxicityPromptsEvaluatingNeural2020,
  title = {{{RealToxicityPrompts}}: {{Evaluating Neural Toxic Degeneration}} in {{Language Models}}},
  shorttitle = {{{RealToxicityPrompts}}},
  author = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
  date = {2020-09-25},
  eprint = {2009.11462},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2009.11462},
  url = {http://arxiv.org/abs/2009.11462},
  urldate = {2024-11-24},
  abstract = {Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning "bad" words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/WZ89PWGR/Gehman et al. - 2020 - RealToxicityPrompts Evaluating Neural Toxic Degeneration in Language Models.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/NWQA67KI/2009.html}
}

@online{gulcehreReinforcedSelfTrainingReST2023,
  title = {Reinforced {{Self-Training}} ({{ReST}}) for {{Language Modeling}}},
  author = {Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and Macherey, Wolfgang and Doucet, Arnaud and Firat, Orhan and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2023-08-21},
  eprint = {2308.08998},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.08998},
  url = {http://arxiv.org/abs/2308.08998},
  urldate = {2023-08-26},
  abstract = {Reinforcement learning from human feedback (RLHF) can improve the quality of large language model's (LLM) outputs by aligning them with human preferences. We propose a simple algorithm for aligning LLMs with human preferences inspired by growing batch reinforcement learning (RL), which we call Reinforced Self-Training (ReST). Given an initial LLM policy, ReST produces a dataset by generating samples from the policy, which are then used to improve the LLM policy using offline RL algorithms. ReST is more efficient than typical online RLHF methods because the training dataset is produced offline, which allows data reuse. While ReST is a general approach applicable to all generative learning settings, we focus on its application to machine translation. Our results show that ReST can substantially improve translation quality, as measured by automated metrics and human evaluation on machine translation benchmarks in a compute and sample-efficient manner.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/2I2YK2IQ/Gulcehre et al. - 2023 - Reinforced Self-Training (ReST) for Language Model.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/EC5YHT6P/2308.html}
}

@online{guptaRethinkingRolePPO,
  title = {Rethinking the {{Role}} of {{PPO}} in {{RLHF}}},
  author = {Gupta, Ritwik and Wu, Tianhao},
  url = {http://bair.berkeley.edu/blog/2023/10/16/p3o/},
  urldate = {2024-10-16},
  abstract = {The BAIR Blog},
  organization = {The Berkeley Artificial Intelligence Research Blog},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/GSEJEJK8/p3o.html}
}

@online{hongORPOMonolithicPreference2024,
  title = {{{ORPO}}: {{Monolithic Preference Optimization}} without {{Reference Model}}},
  shorttitle = {{{ORPO}}},
  author = {Hong, Jiwoo and Lee, Noah and Thorne, James},
  date = {2024-03-14},
  eprint = {2403.07691},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2403.07691},
  urldate = {2024-11-10},
  abstract = {While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\% on \$\textbackslash text\{AlpacaEval\}\_\{2.0\}\$ (Figure 1), 66.19\% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\$\textbackslash alpha\$ (7B) and Mistral-ORPO-\$\textbackslash beta\$ (7B).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/AFYZIHJG/Hong et al. - 2024 - ORPO Monolithic Preference Optimization without Reference Model.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/INRPML28/2403.html}
}

@online{hongORPOMonolithicPreference2024a,
  title = {{{ORPO}}: {{Monolithic Preference Optimization}} without {{Reference Model}}},
  shorttitle = {{{ORPO}}},
  author = {Hong, Jiwoo and Lee, Noah and Thorne, James},
  date = {2024-03-14},
  eprint = {2403.07691},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2403.07691},
  url = {http://arxiv.org/abs/2403.07691},
  urldate = {2024-11-23},
  abstract = {While recent preference alignment algorithms for language models have demonstrated promising results, supervised fine-tuning (SFT) remains imperative for achieving successful convergence. In this paper, we study the crucial role of SFT within the context of preference alignment, emphasizing that a minor penalty for the disfavored generation style is sufficient for preference-aligned SFT. Building on this foundation, we introduce a straightforward and innovative reference model-free monolithic odds ratio preference optimization algorithm, ORPO, eliminating the necessity for an additional preference alignment phase. We demonstrate, both empirically and theoretically, that the odds ratio is a sensible choice for contrasting favored and disfavored styles during SFT across the diverse sizes from 125M to 7B. Specifically, fine-tuning Phi-2 (2.7B), Llama-2 (7B), and Mistral (7B) with ORPO on the UltraFeedback alone surpasses the performance of state-of-the-art language models with more than 7B and 13B parameters: achieving up to 12.20\% on \$\textbackslash text\{AlpacaEval\}\_\{2.0\}\$ (Figure 1), 66.19\% on IFEval (instruction-level loose, Table 6), and 7.32 in MT-Bench (Figure 12). We release code and model checkpoints for Mistral-ORPO-\$\textbackslash alpha\$ (7B) and Mistral-ORPO-\$\textbackslash beta\$ (7B).},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/CAJD6IJK/Hong et al. - 2024 - ORPO Monolithic Preference Optimization without Reference Model.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/EGSY4SUX/2403.html}
}

@video{huggingfaceReinforcementLearningHuman2022,
  entrysubtype = {video},
  title = {Reinforcement {{Learning}} from {{Human Feedback}}: {{From Zero}} to {{chatGPT}}},
  shorttitle = {Reinforcement {{Learning}} from {{Human Feedback}}},
  editor = {{HuggingFace}},
  editortype = {director},
  date = {2022-12-13},
  url = {https://www.youtube.com/watch?v=2MBJOuVq380},
  urldate = {2024-11-13},
  abstract = {In this talk, we will cover the basics of Reinforcement Learning from Human Feedback (RLHF) and how this technology is being used to enable state-of-the-art ML tools like ChatGPT. Most of the talk will be an overview of the interconnected ML models and cover the basics of Natural Language Processing and RL that one needs to understand how RLHF is used on large language models. It will conclude with open question in RLHF. RLHF Blogpost: https://huggingface.co/blog/rlhf The Deep RL Course: https://hf.co/deep-rl-course Slides from this talk: https://docs.google.com/presentation/... Nathan Twitter: ~~/~natolambert~~ Thomas Twitter: ~~/~thomassimonini~~ Nathan Lambert is a Research Scientist at HuggingFace. He received his PhD from the University of California, Berkeley working at the intersection of machine learning and robotics. He was advised by Professor Kristofer Pister in the Berkeley Autonomous Microsystems Lab and Roberto Calandra at Meta AI Research. He was lucky to intern at Facebook AI and DeepMind during his Ph.D. Nathan was was awarded the UC Berkeley EECS Demetri Angelakos Memorial Achievement Award for Altruism for his efforts to better community norms.}
}

@unpublished{InstructionFinetuningRLHF,
  title = {Instruction Finetuning and {{RLHF}} Lecture},
  url = {https://docs.google.com/presentation/d/13Tylt2SvKvBL2hgILy5CmBtPDv3rXlVrQp01OzAe5Xo},
  urldate = {2024-11-11},
  keywords = {To Read}
}

@unpublished{InstructionFinetuningRLHFa,
  title = {Instruction Finetuning and {{RLHF}} Lecture},
  url = {https://docs.google.com/presentation/d/13Tylt2SvKvBL2hgILy5CmBtPDv3rXlVrQp01OzAe5Xo},
  urldate = {2024-11-11}
}

@unpublished{InstructionFinetuningRLHFb,
  title = {Instruction Finetuning and {{RLHF}} Lecture},
  url = {https://docs.google.com/presentation/d/13Tylt2SvKvBL2hgILy5CmBtPDv3rXlVrQp01OzAe5Xo},
  urldate = {2024-11-11}
}

@video{jeremyhowardHackersGuideLanguage2023,
  entrysubtype = {video},
  title = {A {{Hackers}}' {{Guide}} to {{Language Models}}},
  editor = {{Jeremy Howard}},
  editortype = {director},
  date = {2023-09-24},
  url = {https://www.youtube.com/watch?v=jkrNMKz9pWU},
  urldate = {2023-09-29},
  abstract = {In this deeply informative video, Jeremy Howard, co-founder of fast.ai and creator of the ULMFiT approach on which all modern language models (LMs) are based, takes you on a comprehensive journey through the fascinating landscape of LMs. Starting with the foundational concepts, Jeremy introduces the architecture and mechanics that make these AI systems tick. He then delves into critical evaluations of GPT-4, illuminates practical uses of language models in code writing and data analysis, and offers hands-on tips for working with the OpenAI API. The video also provides expert guidance on technical topics such as fine-tuning, decoding tokens, and running private instances of GPT models. As we move further into the intricacies, Jeremy unpacks advanced strategies for model testing and optimization, utilizing tools like GPTQ and Hugging Face Transformers. He also explores the potential of specialized datasets like Orca and Platypus for fine-tuning and discusses cutting-edge trends in Retrieval Augmented Generation and information retrieval. Whether you're new to the field or an established professional, this presentation offers a wealth of insights to help you navigate the ever-evolving world of language models. (The above summary was, of course, created by an LLM!) For the notebook used in this talk, see https://github.com/fastai/lm-hackers. 00:00:00 Introduction \& Basic Ideas of Language Models 00:18:05 Limitations \& Capabilities of GPT-4 00:31:28 AI Applications in Code Writing, Data Analysis \& OCR 00:38:50 Practical Tips on Using OpenAI API 00:46:36 Creating a Code Interpreter with Function Calling 00:51:57 Using Local Language Models \& GPU Options 00:59:33 Fine-Tuning Models \& Decoding Tokens 01:05:37 Testing \& Optimizing Models 01:10:32 Retrieval Augmented Generation 01:20:08 Fine-Tuning Models 01:26:00 Running Models on Macs 01:27:42 Llama.cpp \& Its Cross-Platform Abilities This is an extended version of the keynote given at posit::conf(2023). Thanks to @wolpumba4099 for chapter titles.}
}

@online{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and family=Casas, given=Diego, prefix=de las, useprefix=false and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  date = {2023-10-10},
  eprint = {2310.06825},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2310.06825},
  url = {http://arxiv.org/abs/2310.06825},
  urldate = {2024-11-29},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/EDRPJRIN/Jiang et al. - 2023 - Mistral 7B.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/BKP3FEI2/2310.html}
}

@online{kaufmannSurveyReinforcementLearning2024,
  title = {A {{Survey}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Kaufmann, Timo and Weng, Paul and Bengs, Viktor and Hüllermeier, Eyke},
  date = {2024-04-30},
  eprint = {2312.14925},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2312.14925},
  urldate = {2024-11-13},
  abstract = {Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/6VAXS6FK/Kaufmann et al. - 2024 - A Survey of Reinforcement Learning from Human Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/C6EWKJPI/2312.html}
}

@online{knoxLearningOptimalAdvantage2023,
  title = {Learning {{Optimal Advantage}} from {{Preferences}} and {{Mistaking}} It for {{Reward}}},
  author = {Knox, W. Bradley and Hatgis-Kessell, Stephane and Adalgeirsson, Sigurdur Orn and Booth, Serena and Dragan, Anca and Stone, Peter and Niekum, Scott},
  date = {2023-10-03},
  eprint = {2310.02456},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.02456},
  url = {http://arxiv.org/abs/2310.02456},
  urldate = {2023-10-07},
  abstract = {We consider algorithms for learning reward functions from human preferences over pairs of trajectory segments, as used in reinforcement learning from human feedback (RLHF). Most recent work assumes that human preferences are generated based only upon the reward accrued within those segments, or their partial return. Recent work casts doubt on the validity of this assumption, proposing an alternative preference model based upon regret. We investigate the consequences of assuming preferences are based upon partial return when they actually arise from regret. We argue that the learned function is an approximation of the optimal advantage function, \$\textbackslash hat\{A\textasciicircum *\_r\}\$, not a reward function. We find that if a specific pitfall is addressed, this incorrect assumption is not particularly harmful, resulting in a highly shaped reward function. Nonetheless, this incorrect usage of \$\textbackslash hat\{A\textasciicircum *\_r\}\$ is less desirable than the appropriate and simpler approach of greedy maximization of \$\textbackslash hat\{A\textasciicircum *\_r\}\$. From the perspective of the regret preference model, we also provide a clearer interpretation of fine tuning contemporary large language models with RLHF. This paper overall provides insight regarding why learning under the partial return preference model tends to work so well in practice, despite it conforming poorly to how humans give preferences.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6,I.2.8},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/E9527Y29/Knox et al. - 2023 - Learning Optimal Advantage from Preferences and Mi.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/EBPX9D96/2310.html}
}

@online{lambertAlignmentCeilingObjective2023,
  title = {The {{Alignment Ceiling}}: {{Objective Mismatch}} in {{Reinforcement Learning}} from {{Human Feedback}}},
  shorttitle = {The {{Alignment Ceiling}}},
  author = {Lambert, Nathan and Calandra, Roberto},
  date = {2023-10-31},
  eprint = {2311.00168},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.00168},
  url = {http://arxiv.org/abs/2311.00168},
  urldate = {2023-11-02},
  abstract = {Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to prompt and more capable in complex settings. RLHF at its core is providing a new toolkit to optimize LLMs other than next-token prediction, enabling the integration of qualitative training goals. The attempted match between user preferences and downstream performance, which happens in a learned reward model, results in an optimization landscape where training and evaluation metrics can appear correlated. The apparent correlation can lead to unexpected behaviors and stories of "too much RLHF." In RLHF, challenges emerge because the following sub-modules are not consistent with each other: the reward model training, the policy model training, and the policy model evaluation. This mismatch results in models that sometimes avoid user requests for false safety flags, are difficult to steer to an intended characteristic, or always answer in a specific style. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model score and downstream performance drives the objective mismatch issue. In this paper, we illustrate the cause of this issue, reviewing relevant literature from model-based reinforcement learning, and discuss relevant solutions to encourage further research. By solving objective mismatch in RLHF, the LLMs of the future will be more precisely aligned to user instructions for both safety and helpfulness.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/5XAW2RTQ/Lambert og Calandra - 2023 - The Alignment Ceiling Objective Mismatch in Reinf.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/EVIK8XFG/2311.html}
}

@article{lambertBasicsReinforcementLearning,
  title = {The {{Basics}} of {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Lambert, Nathan},
  abstract = {Reinforcement learning from human feedback (RLHF) has become an important technical and storytelling tool to the deploy of the lastest machine learning systems. In this book, we hope to give a gentle introduction to the core methods for people with some level of quantitative background. The book starts with the origins of RLHF – both in recent literature and in a convergence of disparate fields of science in economics, philosophy, and optimal control. We then set the stage with definitions, problem formulation, data collection, and other common math used in the literature. We detail the detail the popular algorithms and future frontiers of RLHF.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/USU7JQ6K/Lambert - The Basics of Reinforcement Learning from Human Fe.pdf}
}

@online{lambertEntangledPreferencesHistory2023a,
  title = {Entangled {{Preferences}}: {{The History}} and {{Risks}} of {{Reinforcement Learning}} and {{Human Feedback}}},
  shorttitle = {Entangled {{Preferences}}},
  author = {Lambert, Nathan and Gilbert, Thomas Krendl and Zick, Tom},
  date = {2023-10-20},
  eprint = {2310.13595},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.13595},
  url = {http://arxiv.org/abs/2310.13595},
  urldate = {2023-11-06},
  abstract = {Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF's foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/JMAUE2BJ/Lambert et al. - 2023 - Entangled Preferences The History and Risks of Re.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/HEG2JN52/2310.html}
}

@online{lambertHistoryRisksReinforcement2023,
  title = {The {{History}} and {{Risks}} of {{Reinforcement Learning}} and {{Human Feedback}}},
  author = {Lambert, Nathan and Gilbert, Thomas Krendl and Zick, Tom},
  date = {2023-11-28},
  eprint = {2310.13595},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.13595},
  urldate = {2024-08-13},
  abstract = {Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) easier to use and more effective. A core piece of the RLHF process is the training and utilization of a model of human preferences that acts as a reward function for optimization. This approach, which operates at the intersection of many stakeholders and academic disciplines, remains poorly understood. RLHF reward models are often cited as being central to achieving performance, yet very few descriptors of capabilities, evaluations, training methods, or open-source models exist. Given this lack of information, further study and transparency is needed for learned RLHF reward models. In this paper, we illustrate the complex history of optimizing preferences, and articulate lines of inquiry to understand the sociotechnical context of reward models. In particular, we highlight the ontological differences between costs, rewards, and preferences at stake in RLHF’s foundations, related methodological tensions, and possible research directions to improve general understanding of how reward models function.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computers and Society,To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/CMWSCSQS/Lambert et al. - 2023 - The History and Risks of Reinforcement Learning an.pdf}
}

@article{lambertReinforcementLearningHumana,
  title = {Reinforcement {{Learning}} from {{Human Feedback}}},
  author = {Lambert, Nathan and Ustalov, Dmitry},
  langid = {english},
  keywords = {To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/MP2E3PVT/Lambert og Ustalov - Reinforcement Learning from Human Feedback.pdf}
}

@article{lambertReinforcementLearningHumanb,
  title = {Reinforcement {{Learning}} from {{Human Feedback}}},
  author = {Lambert, Nathan and Ustalov, Dmitry},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/ZXWZYKSC/Lambert og Ustalov - Reinforcement Learning from Human Feedback.pdf}
}

@online{lambertRLHF201Nathan2024,
  title = {{{RLHF}} 201 - with {{Nathan Lambert}} of {{AI2}} and {{Interconnects}}},
  author = {Lambert, Nathan},
  date = {2024-11-11},
  url = {https://www.latent.space/p/rlhf-201},
  urldate = {2024-11-13},
  abstract = {Back to foundations! The origins of RLHF, sociology's influence on it, the tension between human vs synthetic data, and emerging research in the field},
  langid = {english},
  keywords = {To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/5QVM2Z9M/rlhf-201.html}
}

@online{lambertRLHFLearningResources2023,
  title = {{{RLHF}} Learning Resources in 2024},
  author = {Lambert, Nathan},
  date = {2023-11-24},
  url = {https://www.interconnects.ai/p/rlhf-resources},
  urldate = {2024-08-08},
  abstract = {A list for beginners and wannabe experts and everyone in between.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/URHWU4VK/rlhf-resources.html}
}

@online{lambertRLHFLearningResources2023a,
  title = {{{RLHF}} Learning Resources in 2024},
  author = {Lambert, Nathan},
  date = {2023-11-24},
  url = {https://www.interconnects.ai/p/rlhf-resources},
  urldate = {2024-11-10},
  abstract = {A list for beginners and wannabe experts and everyone in between.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/LZCHMR8M/rlhf-resources.html}
}

@article{lambertTULU3Pushing,
  title = {{{TÜLU}} 3: {{Pushing Frontiers}} in {{Open Language Model Post-Training}}},
  author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Xinxi and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
  abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/5KCKQBTS/Lambert et al. - TÜLU 3 Pushing Frontiers in Open Language Model Post-Training.pdf}
}

@online{lambertTULU3Pushing2024,
  title = {{{TÜLU}} 3: {{Pushing Frontiers}} in {{Open Language Model Post-Training}}},
  shorttitle = {{{TÜLU}} 3},
  author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V. and Liu, Alisa and Dziri, Nouha and Lyu, Shane and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D. and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A. and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
  date = {2024-11-22},
  eprint = {2411.15124},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2411.15124},
  url = {http://arxiv.org/abs/2411.15124},
  urldate = {2024-11-27},
  abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\textbackslash "ULU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\textbackslash "ULU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\textbackslash "ULU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance. In addition to the T\textbackslash "ULU 3 model weights and demo, we release the complete recipe -- including datasets for diverse core skills, a robust toolkit for data curation and evaluation, the training code and infrastructure, and, most importantly, a detailed report for reproducing and further adapting the T\textbackslash "ULU 3 approach to more domains.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/UJE8LHKB/Lambert et al. - 2024 - TÜLU 3 Pushing Frontiers in Open Language Model Post-Training.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/ACHZJNK7/2411.html}
}

@article{lambertTULU3Pushinga,
  title = {{{TÜLU}} 3: {{Pushing Frontiers}} in {{Open Language Model Post-Training}}},
  author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Xinxi and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
  abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/ULJ8FBWS/Lambert et al. - TÜLU 3 Pushing Frontiers in Open Language Model Post-Training.pdf}
}

@article{lambertTULU3Pushingb,
  title = {{{TÜLU}} 3: {{Pushing Frontiers}} in {{Open Language Model Post-Training}}},
  author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Xinxi and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
  abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce TÜLU 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. TÜLU 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With TÜLU 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/LCJUU3K2/Lambert et al. - TÜLU 3 Pushing Frontiers in Open Language Model Post-Training.pdf}
}

@online{lbonneMaximeLabonneFinetune2024,
  title = {Maxime {{Labonne}} - {{Fine-tune Mistral-7b}} with {{Direct Preference Optimization}}},
  author = {Lbonne, Maxime},
  date = {2024-01-01},
  url = {https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html},
  urldate = {2024-11-23},
  abstract = {Boosting the performance of your supervised fine-tuned models},
  langid = {english},
  organization = {Maxime Labonne},
  keywords = {To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/97HYYKPG/Fine_tune_Mistral_7b_with_DPO.html}
}

@online{leeRLAIFVsRLHF2024,
  title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},
  shorttitle = {{{RLAIF}} vs. {{RLHF}}},
  author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
  date = {2024-09-03},
  eprint = {2309.00267},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2309.00267},
  urldate = {2024-11-03},
  abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards "self-improvement" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/HDVJ2N86/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/WTUZ9JTI/2309.html}
}

@online{leeRLAIFVsRLHF2024a,
  title = {{{RLAIF}} vs. {{RLHF}}: {{Scaling Reinforcement Learning}} from {{Human Feedback}} with {{AI Feedback}}},
  shorttitle = {{{RLAIF}} vs. {{RLHF}}},
  author = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and Prakash, Sushant},
  date = {2024-09-03},
  eprint = {2309.00267},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2309.00267},
  urldate = {2024-11-10},
  abstract = {Reinforcement learning from human feedback (RLHF) has proven effective in aligning large language models (LLMs) with human preferences, but gathering high-quality preference labels is expensive. RL from AI Feedback (RLAIF), introduced in Bai et al., offers a promising alternative that trains the reward model (RM) on preferences generated by an off-the-shelf LLM. Across the tasks of summarization, helpful dialogue generation, and harmless dialogue generation, we show that RLAIF achieves comparable performance to RLHF. Furthermore, we take a step towards "self-improvement" by demonstrating that RLAIF can outperform a supervised fine-tuned baseline even when the AI labeler is the same size as the policy, or even the exact same checkpoint as the initial policy. Finally, we introduce direct-RLAIF (d-RLAIF) - a technique that circumvents RM training by obtaining rewards directly from an off-the-shelf LLM during RL, which achieves superior performance to canonical RLAIF. Our results suggest that RLAIF can achieve performance on-par with using human feedback, offering a potential solution to the scalability limitations of RLHF.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/B92IHWY3/Lee et al. - 2024 - RLAIF vs. RLHF Scaling Reinforcement Learning from Human Feedback with AI Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/FS9NN7ZH/2309.html}
}

@online{liAgentWhatWhy2023,
  type = {Substack newsletter},
  title = {Agent: {{What}}, {{Why}}, {{How}}.},
  shorttitle = {Agent},
  author = {Li, Yuxi},
  date = {2023-08-31},
  url = {https://yuxili.substack.com/p/agent-what-why-how},
  urldate = {2023-09-01},
  abstract = {Agent is a core concept in AI. As Large/Language Models (LMs) become popular, people are talking about building autonomous agents based on LMs. Executive Summary The learner and decision maker is called the agent. The whole field of AI is built around agents.},
  organization = {Yuxi’s Substack}
}

@online{linTruthfulQAMeasuringHow2022,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  shorttitle = {{{TruthfulQA}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  date = {2022-05-08},
  eprint = {2109.07958},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2109.07958},
  url = {http://arxiv.org/abs/2109.07958},
  urldate = {2024-11-17},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/A8CNQLBY/Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human Falsehoods.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/FIWBQCRD/2109.html}
}

@online{madaanMemoryassistedPromptEditing2023,
  title = {Memory-Assisted Prompt Editing to Improve {{GPT-3}} after Deployment},
  author = {Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  date = {2023-02-18},
  eprint = {2201.06009},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.06009},
  urldate = {2024-09-24},
  abstract = {Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret "What word is similar to good?" to mean a homophone, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. Code, data, and instructions to implement MEMPROMPT for a new task at https://www.memprompt.com/.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/VGH3DIBE/Madaan et al. - 2023 - Memory-assisted prompt editing to improve GPT-3 af.pdf}
}

@online{minaeeLargeLanguageModels2024,
  title = {Large {{Language Models}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}}},
  author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  date = {2024-02-20},
  eprint = {2402.06196},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2402.06196},
  urldate = {2024-11-21},
  abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \textbackslash cite\{kaplan2020scaling,hoffmann2022training\}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/Q9TRRPHF/Minaee et al. - 2024 - Large Language Models A Survey.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/ZVKT7X95/2402.html}
}

@online{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-06-16},
  eprint = {1602.01783},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2024-11-21},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/W3JJV6QT/Mnih et al. - 2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/CF4NQ63H/1602.html}
}

@online{nguyenPredictingStringsLanguage2024,
  title = {Predicting from {{Strings}}: {{Language Model Embeddings}} for {{Bayesian Optimization}}},
  shorttitle = {Predicting from {{Strings}}},
  author = {Nguyen, Tung and Zhang, Qiuyi and Yang, Bangding and Lee, Chansoo and Bornschein, Jorg and Miao, Yingjie and Perel, Sagi and Chen, Yutian and Song, Xingyou},
  date = {2024-10-15},
  eprint = {2410.10190},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2410.10190},
  url = {http://arxiv.org/abs/2410.10190},
  urldate = {2024-10-17},
  abstract = {Bayesian Optimization is ubiquitous in the field of experimental design and blackbox optimization for improving search efficiency, but has been traditionally restricted to regression models which are only applicable to fixed search spaces and tabular input features. We propose Embed-then-Regress, a paradigm for applying in-context regression over string inputs, through the use of string embedding capabilities of pretrained language models. By expressing all inputs as strings, we are able to perform general-purpose regression for Bayesian Optimization over various domains including synthetic, combinatorial, and hyperparameter optimization, obtaining comparable results to state-of-the-art Gaussian Process-based algorithms. Code can be found at https://github.com/google-research/optformer/tree/main/optformer/embed\_then\_regress.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/VPL5N3A9/Nguyen et al. - 2024 - Predicting from Strings Language Model Embeddings.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/ESERHGPZ/2410.html}
}

@online{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  date = {2024-03-04},
  eprint = {2303.08774},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2303.08774},
  urldate = {2024-09-24},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/556RNYAQ/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf}
}

@software{OpenaiSummarizefeedback2024,
  title = {Openai/Summarize-from-Feedback},
  date = {2024-10-03T19:09:59Z},
  origdate = {2020-09-02T17:34:05Z},
  url = {https://github.com/openai/summarize-from-feedback},
  urldate = {2024-10-08},
  abstract = {Code for "Learning to summarize from human feedback"},
  organization = {OpenAI}
}

@online{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  date = {2022-03-04},
  eprint = {2203.02155},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-07-19},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/WJLC3J9G/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/IUNF69FF/2203.html}
}

@online{ph.dBasicsReinforcementLearning2024,
  title = {Basics of {{Reinforcement Learning}} for {{LLMs}}},
  author = {Ph.D, Cameron R. Wolfe},
  date = {2024-01-31T07:33:20},
  url = {https://towardsdatascience.com/basics-of-reinforcement-learning-for-llms-d74c5178cd2d},
  urldate = {2024-01-31},
  abstract = {Understanding the problem formulation and basic algorithms for RL},
  langid = {english},
  organization = {Medium},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/XXK5TUZX/basics-of-reinforcement-learning-for-llms-d74c5178cd2d.html}
}

@online{phdLLMTrainingRLHF2023,
  title = {{{LLM Training}}: {{RLHF}} and {{Its Alternatives}}},
  shorttitle = {{{LLM Training}}},
  author = {PhD, Sebastian Raschka},
  date = {2023-04-16},
  url = {https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives},
  urldate = {2024-11-23},
  abstract = {I frequently reference a process called Reinforcement Learning with Human Feedback (RLHF) when discussing LLMs, whether in the research news or tutorials.},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/JJGVVVPI/llm-training-rlhf-and-its-alternatives.html}
}

@video{ProxyObjectivesReinforcement,
  entrysubtype = {video},
  title = {Proxy Objectives in Reinforcement Learning from Human Feedback},
  url = {https://slideslive.com/39004369/proxy-objectives-in-reinforcement-learning-from-human-feedback?ref=og-meta-tags},
  urldate = {2023-09-29},
  langid = {american},
  keywords = {To Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/H4G4GHXJ/proxy-objectives-in-reinforcement-learning-from-human-feedback.html}
}

@online{rafailovDirectPreferenceOptimization2024a,
  title = {Direct {{Preference Optimization}}: {{Your Language Model}} Is {{Secretly}} a {{Reward Model}}},
  shorttitle = {Direct {{Preference Optimization}}},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
  date = {2024-07-29},
  eprint = {2305.18290},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.18290},
  url = {http://arxiv.org/abs/2305.18290},
  urldate = {2024-09-17},
  abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Read},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/TA9TZ3HB/Rafailov et al. - 2024 - Direct Preference Optimization Your Language Mode.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/S2ZQYEYY/2305.html}
}

@online{RawQuestionjsonlHuggingFaceH42023,
  title = {Raw/Question.Jsonl · {{HuggingFaceH4}}/Mt\_bench\_prompts at Main},
  date = {2023-07-03},
  url = {https://huggingface.co/datasets/HuggingFaceH4/mt_bench_prompts/blob/main/raw/question.jsonl},
  urldate = {2024-11-01},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/8QNK7GDC/question.html}
}

@video{ReinforcementLearningHuman,
  entrysubtype = {video},
  title = {Reinforcement {{Learning}} from {{Human Feedback}}: {{A Tutorial}} *},
  shorttitle = {Reinforcement {{Learning}} from {{Human Feedback}}},
  url = {https://slideslive.com/39004357/reinforcement-learning-from-human-feedback-a-tutorial-?ref=og-meta-tags},
  urldate = {2023-09-29},
  langid = {american},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/3M5QTRW8/reinforcement-learning-from-human-feedback-a-tutorial-.html}
}

@online{schmidRLHF2024DPO2024,
  title = {{{RLHF}} in 2024 with {{DPO}} \& {{Hugging Face}}},
  author = {Schmid, Philipp},
  date = {2024-01-23},
  url = {https://www.philschmid.de/dpo-align-llms-in-2024-with-trl},
  urldate = {2024-10-17},
  abstract = {In this blog post you will learn how to align LLMs using Hugging Face TRL and RLHF through Direct Preference Optimization (DPO).},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/H9N2QYVH/dpo-align-llms-in-2024-with-trl.html}
}

@unpublished{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2022-05-06},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  keywords = {Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/5ALJJLRP/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/IAQ7G6RR/1707.html}
}

@article{shannonMathematicalTheoryCommunicationa,
  title = {The {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude and Weaver, Warren},
  langid = {english},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/TSW9JMQX/Shannon og Weaver - The Mathematical Theory of Communication.pdf}
}

@online{stiennonLearningSummarizeHuman2022,
  title = {Learning to Summarize from Human Feedback},
  author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
  date = {2022-02-15},
  eprint = {2009.01325},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2009.01325},
  urldate = {2024-10-08},
  abstract = {As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about—summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts [63] and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles [22], producing summaries nearly as good as the human reference without any news-specific fine-tuning.2 We conduct extensive analyses to understand our human feedback dataset and fine-tuned models.3 We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/29RAJXB9/Stiennon et al. - 2022 - Learning to summarize from human feedback.pdf}
}

@online{TeachLlamasTalk,
  title = {Teach {{Llamas}} to {{Talk}}: {{Recent Progress}} in {{Instruction Tuning}}},
  url = {https://gaotianyu.xyz/blog/2023/11/30/instruction-tuning/},
  urldate = {2023-12-04},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/9UALF8E4/instruction-tuning.html}
}

@online{thoppilanLaMDALanguageModels2022,
  title = {{{LaMDA}}: {{Language Models}} for {{Dialog Applications}}},
  shorttitle = {{{LaMDA}}},
  author = {Thoppilan, Romal and Freitas, Daniel De and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and Li, YaGuang and Lee, Hongrae and Zheng, Huaixiu Steven and Ghafouri, Amin and Menegali, Marcelo and Huang, Yanping and Krikun, Maxim and Lepikhin, Dmitry and Qin, James and Chen, Dehao and Xu, Yuanzhong and Chen, Zhifeng and Roberts, Adam and Bosma, Maarten and Zhao, Vincent and Zhou, Yanqi and Chang, Chung-Ching and Krivokon, Igor and Rusch, Will and Pickett, Marc and Srinivasan, Pranesh and Man, Laichee and Meier-Hellstern, Kathleen and Morris, Meredith Ringel and Doshi, Tulsee and Santos, Renelito Delos and Duke, Toju and Soraker, Johnny and Zevenbergen, Ben and Prabhakaran, Vinodkumar and Diaz, Mark and Hutchinson, Ben and Olson, Kristen and Molina, Alejandra and Hoffman-John, Erin and Lee, Josh and Aroyo, Lora and Rajakumar, Ravi and Butryna, Alena and Lamm, Matthew and Kuzmina, Viktoriya and Fenton, Joe and Cohen, Aaron and Bernstein, Rachel and Kurzweil, Ray and Aguera-Arcas, Blaise and Cui, Claire and Croak, Marian and Chi, Ed and Le, Quoc},
  date = {2022-02-10},
  eprint = {2201.08239},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2201.08239},
  url = {http://arxiv.org/abs/2201.08239},
  urldate = {2024-10-27},
  abstract = {We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/G2L562C6/Thoppilan et al. - 2022 - LaMDA Language Models for Dialog Applications.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/8QQTW6Q7/2201.html}
}

@online{tsakpinisDeepDiveFineTuning2024,
  title = {A {{Deep Dive}} into {{Fine-Tuning}}},
  author = {Tsakpinis, Aris},
  date = {2024-06-03T21:34:11},
  url = {https://towardsdatascience.com/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4860c6d16224},
  urldate = {2024-11-10},
  abstract = {Stepping out of the “comfort zone”{$\mkern1mu$}—{$\mkern1mu$}part 3/3 of a deep-dive into domain adaptation approaches for LLMs},
  langid = {english},
  organization = {Medium},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/UVFQB53B/stepping-out-of-the-comfort-zone-through-domain-adaptation-a-deep-dive-into-dynamic-prompting-4.html}
}

@online{tsakpinisPreferenceAlignmentEveryone2024,
  title = {Preference {{Alignment}} for {{Everyone}}!},
  author = {Tsakpinis, Aris},
  date = {2024-11-08T17:50:36},
  url = {https://towardsdatascience.com/preference-alignment-for-everyone-2563cec4d10e},
  urldate = {2024-11-08},
  abstract = {Frugal RLHF with multi-adapter PPO on Amazon SageMaker},
  langid = {english},
  organization = {Medium},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/5UYLNSCR/preference-alignment-for-everyone-2563cec4d10e.html}
}

@software{tunstallAlignmentHandbook2024,
  title = {The {{Alignment Handbook}}},
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Huang, Shengyi and Rasul, Kashif and Bartolome, Alvaro and M. Rush, Alexander and Wolf, Thomas},
  date = {2024-10-24T06:45:09Z},
  origdate = {2023-08-25T11:35:34Z},
  url = {https://github.com/huggingface/alignment-handbook},
  urldate = {2024-10-24},
  abstract = {Robust recipes to align language models with human and AI preferences},
  version = {0.3.0.dev0}
}

@online{wangSelfTaughtEvaluators2024,
  title = {Self-{{Taught Evaluators}}},
  author = {Wang, Tianlu and Kulikov, Ilia and Golovneva, Olga and Yu, Ping and Yuan, Weizhe and Dwivedi-Yu, Jane and Pang, Richard Yuanzhe and Fazel-Zarandi, Maryam and Weston, Jason and Li, Xian},
  date = {2024-08-05},
  eprint = {2408.02666},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2408.02666},
  urldate = {2024-10-16},
  abstract = {Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/HJNU6ETT/Wang et al. - 2024 - Self-Taught Evaluators.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/YJPJTB6C/2408.html}
}

@online{williamsTargetedManipulationDeception2024,
  title = {Targeted {{Manipulation}} and {{Deception Emerge}} When {{Optimizing LLMs}} for {{User Feedback}}},
  author = {Williams, Marcus and Carroll, Micah and Narang, Adhyyan and Weisser, Constantin and Murphy, Brendan and Dragan, Anca},
  date = {2024-11-04},
  eprint = {2411.02306},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2411.02306},
  urldate = {2024-11-10},
  abstract = {As LLMs become more widely deployed, there is increasing interest in directly optimizing for feedback from end users (e.g. thumbs up) in addition to feedback from paid annotators. However, training to maximize human feedback creates a perverse incentive structure for the AI to resort to manipulative tactics to obtain positive feedback, and some users may be especially vulnerable to such tactics. We study this phenomenon by training LLMs with Reinforcement Learning with simulated user feedback. We have three main findings: 1) Extreme forms of "feedback gaming" such as manipulation and deception can reliably emerge in domains of practical LLM usage; 2) Concerningly, even if only {$<$}2\% of users are vulnerable to manipulative strategies, LLMs learn to identify and surgically target them while behaving appropriately with other users, making such behaviors harder to detect; 3 To mitigate this issue, it may seem promising to leverage continued safety training or LLM-as-judges during training to filter problematic outputs. To our surprise, we found that while such approaches help in some settings, they backfire in others, leading to the emergence of subtler problematic behaviors that would also fool the LLM judges. Our findings serve as a cautionary tale, highlighting the risks of using gameable feedback sources -- such as user feedback -- as a target for RL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/Y9TP6NK6/Williams et al. - 2024 - Targeted Manipulation and Deception Emerge when Optimizing LLMs for User Feedback.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/IT9HNBYV/2411.html}
}

@online{wuThinkingLLMsGeneral2024,
  title = {Thinking {{LLMs}}: {{General Instruction Following}} with {{Thought Generation}}},
  shorttitle = {Thinking {{LLMs}}},
  author = {Wu, Tianhao and Lan, Janice and Yuan, Weizhe and Jiao, Jiantao and Weston, Jason and Sukhbaatar, Sainbayar},
  date = {2024-10-14},
  eprint = {2410.10630},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2410.10630},
  urldate = {2024-10-16},
  abstract = {LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning \& problem-solving tasks.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/3WKUPAI8/Wu et al. - 2024 - Thinking LLMs General Instruction Following with .pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/ZKXR3UFI/2410.html}
}

@unpublished{X5b6Nov2023,
  title = {\textbackslash x5b6 {{Nov}} 2023, {{CoRL LangRob}}\textbackslash x5d {{RLHF}}: {{From LLMs}} to {{Control}}},
  shorttitle = {\textbackslash x5b6 {{Nov}} 2023, {{CoRL LangRob}}\textbackslash x5d {{RLHF}}},
  url = {https://docs.google.com/presentation/d/15i_7iqyUJwDMtyzzzSn83JLETET4Lz2Y7e4sDCAZhfM},
  urldate = {2023-11-07}
}

@unpublished{X5b7Dec2023,
  title = {\textbackslash x5b7 {{Dec}} 2023, {{SC4AI}}\textbackslash x5d {{Quick History}} and {{Risks}} of {{RLHF}}},
  url = {https://docs.google.com/presentation/d/1kBEKvHBugXE5tSnwp2_kQ1VdV5bOP_LveY6gkTe_Vco},
  urldate = {2024-08-13},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/N7EPT38U/x5b7 Dec 2023, SC4AIx5d Quick History and Risks .pdf}
}

@online{xuPerfectBlendRedefining2024,
  title = {The {{Perfect Blend}}: {{Redefining RLHF}} with {{Mixture}} of {{Judges}}},
  shorttitle = {The {{Perfect Blend}}},
  author = {Xu, Tengyu and Helenowski, Eryk and Sankararaman, Karthik Abinav and Jin, Di and Peng, Kaiyan and Han, Eric and Nie, Shaoliang and Zhu, Chen and Zhang, Hejia and Zhou, Wenxuan and Zeng, Zhouhao and He, Yun and Mandyam, Karishma and Talabzadeh, Arya and Khabsa, Madian and Cohen, Gabriel and Tian, Yuandong and Ma, Hao and Wang, Sinong and Fang, Han},
  date = {2024-09-30},
  eprint = {2409.20370},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.2409.20370},
  url = {http://arxiv.org/abs/2409.20370},
  urldate = {2024-11-26},
  abstract = {Reinforcement learning from human feedback (RLHF) has become the leading approach for fine-tuning large language models (LLM). However, RLHF has limitations in multi-task learning (MTL) due to challenges of reward hacking and extreme multi-objective optimization (i.e., trade-off of multiple and/or sometimes conflicting objectives). Applying RLHF for MTL currently requires careful tuning of the weights for reward model and data combinations. This is often done via human intuition and does not generalize. In this work, we introduce a novel post-training paradigm which we called Constrained Generative Policy Optimization (CGPO). The core of CGPO is Mixture of Judges (MoJ) with cost-efficient constrained policy optimization with stratification, which can identify the perfect blend in RLHF in a principled manner. It shows strong empirical results with theoretical guarantees, does not require extensive hyper-parameter tuning, and is plug-and-play in common post-training pipelines. Together, this can detect and mitigate reward hacking behaviors while reaching a pareto-optimal point across an extremely large number of objectives. Our empirical evaluations demonstrate that CGPO significantly outperforms standard RLHF algorithms like PPO and DPO across various tasks including general chat, STEM questions, instruction following, and coding. Specifically, CGPO shows improvements of 7.4\% in AlpacaEval-2 (general chat), 12.5\% in Arena-Hard (STEM \& reasoning), and consistent gains in other domains like math and coding. Notably, PPO, while commonly used, is prone to severe reward hacking in popular coding benchmarks, which CGPO successfully addresses. This breakthrough in RLHF not only tackles reward hacking and extreme multi-objective optimization challenges but also advances the state-of-the-art in aligning general-purpose LLMs for diverse applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/AX2MICXC/Xu et al. - 2024 - The Perfect Blend Redefining RLHF with Mixture of Judges.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/DVYP7GNF/2409.html}
}

@online{yadkoriBelieveNotBelieve2024,
  title = {To {{Believe}} or {{Not}} to {{Believe Your LLM}}},
  author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and György, András and Szepesvári, Csaba},
  date = {2024-07-17},
  eprint = {2406.02543},
  eprinttype = {arXiv},
  url = {http://arxiv.org/abs/2406.02543},
  urldate = {2024-10-24},
  abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/X3WLXH9H/Yadkori et al. - 2024 - To Believe or Not to Believe Your LLM.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/9Y29MXJV/2406.html}
}

@online{yangFoundationModelsDecision2023,
  title = {Foundation {{Models}} for {{Decision Making}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Foundation {{Models}} for {{Decision Making}}},
  author = {Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
  date = {2023-03-07},
  eprint = {2303.04129},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.04129},
  url = {http://arxiv.org/abs/2303.04129},
  urldate = {2023-08-31},
  abstract = {Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/WRJVYGVC/Yang et al. - 2023 - Foundation Models for Decision Making Problems, M.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/YXSL7FJ3/2303.html}
}

@online{zhengSecretsRLHFLarge2023,
  title = {Secrets of {{RLHF}} in {{Large Language Models Part I}}: {{PPO}}},
  shorttitle = {Secrets of {{RLHF}} in {{Large Language Models Part I}}},
  author = {Zheng, Rui and Dou, Shihan and Gao, Songyang and Shen, Wei and Wang, Binghai and Liu, Yan and Jin, Senjie and Liu, Qin and Xiong, Limao and Chen, Lu and Xi, Zhiheng and Zhou, Yuhao and Xu, Nuo and Lai, Wenbin and Zhu, Minghao and Weng, Rongxiang and Cheng, Wensen and Chang, Cheng and Yin, Zhangyue and Hua, Yuan and Huang, Haoran and Sun, Tianxiang and Yan, Hang and Gui, Tao and Zhang, Qi and Qiu, Xipeng and Huang, Xuanjing},
  date = {2023-07-10},
  eprint = {2307.04964},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.04964},
  url = {http://arxiv.org/abs/2307.04964},
  urldate = {2023-07-14},
  abstract = {Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbackslash textbf\{reward models\} to measure human preferences, \textbackslash textbf\{Proximal Policy Optimization\} (PPO) to optimize policy model outputs, and \textbackslash textbf\{process supervision\} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/VAP8CKDX/Zheng et al. - 2023 - Secrets of RLHF in Large Language Models Part I P.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/NK8DD9KG/2307.html}
}

@online{zhi-xuanPreferencesAIAlignment2024,
  title = {Beyond {{Preferences}} in {{AI Alignment}}},
  author = {Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and Ashton, Hal},
  date = {2024-08-29},
  eprint = {2408.16984},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2408.16984},
  url = {http://arxiv.org/abs/2408.16984},
  urldate = {2024-09-09},
  abstract = {The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/8HLSP6PM/Zhi-Xuan et al. - 2024 - Beyond Preferences in AI Alignment.pdf;/home/peyman/snap/zotero-snap/common/Zotero/storage/QUFS5CT5/2408.html}
}

@online{zotero-2992,
  title = {About},
  url = {https://huggingface.co/docs/leaderboards/open_llm_leaderboard/about},
  urldate = {2024-11-23},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/home/peyman/snap/zotero-snap/common/Zotero/storage/WH32T9T6/about.html}
}
