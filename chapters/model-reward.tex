\section{Modeling Human Feedback into Reward Signal}
\label{sec:model-reward}

In scope of the reinforcement learning, an essential component is the  
reward model, which provides feedback to the agent (pre-trained LLMs). 
In other words, we need to communicate to the agent on what is "good" answer 
and what is "bad" answer. However, this alone is not enough. We need to assign scalar 
values to the reward to quantify the quality of the agent's output. 
Therefore, the goal of this chapter is to build a "Reward Model" that can
assign scalar values to the model's output \cite{christianoDeepReinforcementLearning2017}.
To build a reward model, we need preference data. Below is an overview of the process of gathering preference data. \\


\textbf{Preference Data:}\\

Preference data is defined as, for a given prompt $x$, we have two possible completions from the language model, $y_1$ and $y_2$. We ask human labelers to select the better completion. One of the completions is preferred over the other. This preference data is used to train the reward model. Consequently, the training dataset consists of high-quality examples in the format (prompt, winning\_response, losing\_response)

In the case of InstructGPT \cite{ouyangTrainingLanguageModels2022}, the preference data is approximately 50,000 prompts. Each prompt is associated with 4 to 9 responses (text completions), forming between 6 and 36 pairs of (prompt, winning\_response, losing\_response). This results in a total of 300K to 1.8M training examples in the preference data.


Anthropic's Constitutional AI framework \footnote{which is suspected to be the backbone of Claude}provides a different example. It includes 318K comparisons (total number of data points in preference data), of which 135K are generated by humans and 183K are generated by AI. Anthropic has also released an older version of their dataset (hh-rlhf), which consists of approximately 170K comparisons.


\subsection{Reward Model} \label{subsec:reward-model}

The reward model is defined as $r_{\theta}(x,\hat{y})$, where $x$ is the prompt and $\hat{y}$ 
is the large language model's output. From the training data (preference data), we have the data in format of (prompt, winning\_response, losing\_response). The preference data is in the format of $(x, \hat{y}_w, \hat{y}_l)$, where:
\begin{itemize}
    \item $x$: Prompt
    \item $\hat{y}_w$: winning response
    \item $\hat{y}_l$: losing response
\end{itemize}

The reward model is trained to assign a higher reward to the winning response ($\hat{y}_w$) and a lower reward to the losing response ($\hat{y}_w$). We define:

\begin{equation}
  \begin{cases}
        s_w=r_{\theta}(x, \hat{y}_w): & \text{reward model score for winning response} \\
        s_l=r_{\theta}(x, \hat{y}_l): & \text{reward model score for losing response}
    \end{cases}
\end{equation}

Then we employ the following loss function:

\begin{equation}
    \mathcal{L}(\theta) = - \ \mathbb{E}_{(x, \hat{y}_w, \hat{y}_l)} \left[ \log(
    \sigma(s_w - s_l) \right] \label{eq:loss}
\end{equation}

Where $\sigma$ is the sigmoid function. The sigmoid function is defined as:

\begin{equation}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation}

The backpropagation algorithm is used to minimize the loss function 
$\mathcal{L}(\theta)$ (Equation \ref{eq:loss}) and find the optimal parameters $\theta$. Essentially, by minimizing the loss function, the reward model learns to consistently assign higher scores to the winning responses compared to the losing ones.