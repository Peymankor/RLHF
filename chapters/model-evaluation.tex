\section{Evaluating RLHF-Optimized Models}
\label{sec:model-evaluation}

In this section, we discuss the methodologies for evaluating models trained using Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO) process. The evaluation process is crucial to determine how well the model is performing on various metrics. This section explores two primary evaluation methodologies:
\begin{itemize}
    \item Human Evaluation
    \item Public NLP Benchmarks
\end{itemize} 

Each of these methods is discussed in detail below.

\subsection{Human Evaluation}

In human evaluation, human judges are employed to evaluate the quality of the responses generated by the model. This is done by asking human judges (labelers) to rate the responses generated by the model on a scale, or compare them with outputs from baseline models. For example, in the InstructGPT paper \cite{ouyangTrainingLanguageModels2022}, human judges rated responses generated by RLHF-optimized models against the baseline model (175B-parameter supervised fine-tuned (SFT) model).
Across all model sizes (1.3B, 6B, 175B), models trained with RLHF consistently outperformed baseline models like SFT and GPT-3, as shown in Figure~\ref{fig:human-evaluation}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/instructgptgraph.png}
    \caption{Comparison of model performance evaluated by human judges.}
    \label{fig:human-evaluation}
\end{figure}

\subsection{Public NLP Benchmarks}

To evaluate specific improvements achieved through RLHF or DPO training (after Supervised Fine-Tuning stage), public NLP benchmarks are used as metrics. In this section we explore two benchmark datasets that assess model performance in terms of \textbf{truthfulness} and 
\textbf{toxicity}.

\subsubsection{Truthfulness Benchmark}

The TruthfulQA dataset \cite{linTruthfulQAMeasuringHow2022} comprises 817 questions across 38 categories, such as health, law, finance, and politics. The dataset is a bit tricky as it include questions where human respondents might provide false answers due to misconceptions or misinformation. 

Figure~\ref{fig:truthfulqa-BENCHMARK} shows examples of questions from the TruthfulQA dataset and the false answers generated by GPT-3. Results presented in \cite{ouyangTrainingLanguageModels2022} show demonstrates that RLHF-trained models show measurable improvements in truthfulness over GPT-3. These results have been shown in the Figure~\ref{fig:truthfulqa-INSTRUCTGPT}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/image_truthfullnes.png}
    \caption{Example of questions from the TruthfulQA benchmark and false answers generated by GPT-3.}
    \label{fig:truthfulqa-BENCHMARK}
\end{figure}



\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/truthfullnes_image.png}
    \caption{Evaluation of InstructGPT on TruthfulQA benchmark, orange and red colors represent RLHF-trained models.}
    \label{fig:truthfulqa-INSTRUCTGPT}
\end{figure}


\subsubsection{Toxicity Benchmark}

To benchmark the toxicity of the responses generated by the model, the RealToxicityPrompts dataset \cite{gehmanRealToxicityPromptsEvaluatingNeural2020} is used. RLHF-trained models, such as InstructGPT, exhibit slight improvements in reducing toxicity compared to GPT-3, particularly when prompted to produce "respectful responses".
although the improvements in bias reduction for InstructGPT remain minimal.